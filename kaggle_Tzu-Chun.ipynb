{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use my code  \n",
    "1. run the first block to define the functions.\n",
    "2. run the second block to check cross validated performance. Change the last arugment for function \"set_classifier\" \"cv_performance_assessment\" to choose the classifier and feature extracting method you want to use.\n",
    "3. run the third block to generate the csv file to upload to kaggle. Change the code in line 15~21 to choose the classifier and feature extracting method you want to use.  \n",
    "*remember to change the directories if you put the data in the different place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "'''Sample script for solar array image classification\n",
    "\n",
    "Author:       Kyle Bradbury, modified by Tzu-Chun\n",
    "Date:         January 30, 2018, February 16, 2020\n",
    "Organization: Duke University Energy Initiative, Duke MIDS student\n",
    "'''\n",
    "\n",
    "'''\n",
    "Import the packages needed for classification\n",
    "'''\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import sklearn.metrics as metrics\n",
    "plt.close()\n",
    "\n",
    "#Install TensorFlow GPU: https://blog.quantinsti.com/install-tensorflow-gpu/\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import svm\n",
    "\n",
    "# keras (CNN)\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten\n",
    "\n",
    "# HOG\n",
    "from skimage.feature import hog\n",
    "from skimage import data, exposure\n",
    "\n",
    "'''\n",
    "Set directory parameters\n",
    "'''\n",
    "# Set the directories for the data and the CSV files that contain ids/labels\n",
    "dir_train_images  = './training/'\n",
    "dir_test_images   = './testing/'\n",
    "dir_train_labels  = './labels_training.csv'\n",
    "dir_test_ids      = './sample_submission.csv'\n",
    "\n",
    "'''\n",
    "Include the functions used for loading, preprocessing, features extraction, \n",
    "classification, and performance evaluation\n",
    "'''\n",
    "\n",
    "def load_data(dir_data, dir_labels, training=True):\n",
    "    ''' Load each of the image files into memory \n",
    "\n",
    "    While this is feasible with a smaller dataset, for larger datasets,\n",
    "    not all the images would be able to be loaded into memory\n",
    "\n",
    "    When training=True, the labels are also loaded\n",
    "    '''\n",
    "    labels_pd = pd.read_csv(dir_labels)\n",
    "    ids       = labels_pd.id.values\n",
    "    data      = []\n",
    "    for identifier in ids:\n",
    "        fname     = dir_data + identifier.astype(str) + '.tif'\n",
    "        image     = mpl.image.imread(fname)\n",
    "        data.append(image)\n",
    "    data = np.array(data) # Convert to Numpy array\n",
    "    if training:\n",
    "        labels = labels_pd.label.values\n",
    "        return data, labels\n",
    "    else:\n",
    "        return data, ids\n",
    "\n",
    "def preprocess_and_extract_features(data):\n",
    "    '''Preprocess data and extract features\n",
    "    \n",
    "    Preprocess: normalize, scale, repair\n",
    "    Extract features: transformations and dimensionality reduction\n",
    "    '''\n",
    "    # Here, we do something trivially simple: we take the average of the RGB\n",
    "    # values to produce a grey image, transform that into a vector, then\n",
    "    # extract the mean and standard deviation as features.\n",
    "    \n",
    "    # Make the image grayscale\n",
    "    data = np.mean(data, axis=3)\n",
    "    \n",
    "    # Vectorize the grayscale matrices\n",
    "    vectorized_data = data.reshape(data.shape[0],-1)\n",
    "    \n",
    "    # extract the mean and standard deviation of each sample as features\n",
    "    feature_mean = np.mean(vectorized_data,axis=1)\n",
    "    feature_std  = np.std(vectorized_data,axis=1)\n",
    "    \n",
    "    # Combine the extracted features into a single feature vector\n",
    "    features = np.stack((feature_mean,feature_std),axis=-1)\n",
    "    \n",
    "    return features\n",
    "\n",
    "def preprocess_and_extract_features_HOG(data):\n",
    "    '''Preprocess data and extract features\n",
    "    \n",
    "    1. (optional) global image normalisation\n",
    "    2. computing the gradient image in x and y\n",
    "    3. computing gradient histograms\n",
    "    4. normalising across blocks\n",
    "    5. flattening into a feature vector\n",
    "    '''\n",
    "    # run first image for computaional purpose\n",
    "    features, hog_image = hog(data[0], orientations=8, pixels_per_cell=(16, 16),\n",
    "                                        cells_per_block=(1, 1), multichannel=True, feature_vector = True, visualize=True)\n",
    "    features =  features.reshape(-1,1)\n",
    "    hog_image = [hog_image]\n",
    "    \n",
    "    # load the image one by one\n",
    "    for img in data:\n",
    "        # Extract Histogram of Oriented Gradients (HOG) for a given image.\n",
    "        f, img = hog(img, orientations=8, pixels_per_cell=(16, 16),\n",
    "                                        cells_per_block=(1, 1), multichannel=True, feature_vector = True, visualize=True)\n",
    "        # concatenate the outcomes \n",
    "        features = np.concatenate((features, f.reshape(-1,1)), axis = 1)\n",
    "        hog_image = np.concatenate((hog_image, [img]), axis = 0 )\n",
    "        \n",
    "\n",
    "    # delete the duplicated first column and transpose the array\n",
    "    features =  np.delete(features, 0, 1)\n",
    "    features = np.transpose(features)\n",
    "    # delete the duplicated first column and reshape\n",
    "    hog_image = np.delete(hog_image, 0, 0)\n",
    "    hog_image = hog_image.reshape(len(hog_image), 101, 101, 1)\n",
    "    \n",
    "    return features, hog_image\n",
    "\n",
    "def set_classifier(c = 0):\n",
    "    '''Shared function to select the classifier for both performance evaluation\n",
    "    and testing\n",
    "    '''\n",
    "    if c == 0:\n",
    "        return KNeighborsClassifier(n_neighbors=7)\n",
    "    if c == 1:\n",
    "        model = Sequential()\n",
    "        #add model layers\n",
    "        model.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(101,101,1)))\n",
    "        model.add(Conv2D(32, kernel_size=3, activation='relu'))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(2, activation='softmax'))\n",
    "        #compile model using accuracy to measure model performance\n",
    "        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])    \n",
    "        return model\n",
    "    if c == 2:\n",
    "        return svm.SVC(probability = True)\n",
    "\n",
    "def cv_performance_assessment(X,y,k,clf, f=0):\n",
    "    '''Cross validated performance assessment\n",
    "    \n",
    "    X   = training data\n",
    "    y   = training labels\n",
    "    k   = number of folds for cross validation\n",
    "    clf = classifier to use\n",
    "    f = feature extract method to use\n",
    "    \n",
    "    Divide the training data into k folds of training and validation data. \n",
    "    For each fold the classifier will be trained on the training data and\n",
    "    tested on the validation data. The classifier prediction scores are \n",
    "    aggregated and output\n",
    "    '''\n",
    "    # Establish the k folds\n",
    "    prediction_scores = np.empty(y.shape[0],dtype='object')\n",
    "    kf = StratifiedKFold(n_splits=k, shuffle=True)\n",
    "    for train_index, val_index in kf.split(X, y):\n",
    "        # Extract the training and validation data for this fold\n",
    "        X_train, X_val   = X[train_index], X[val_index]\n",
    "        y_train          = y[train_index]\n",
    "        \n",
    "        X_train_features = []\n",
    "        X_val_features = []\n",
    "        \n",
    "        # if clf is not CNN\n",
    "        if isinstance(clf, Sequential) != True:\n",
    "            # extract features depends on the method chose\n",
    "            if f == 0:\n",
    "                X_train_features = preprocess_and_extract_features(X_train)\n",
    "                X_val_features   = preprocess_and_extract_features(X_val)\n",
    "            if f == 1:\n",
    "                X_train_features, _ = preprocess_and_extract_features_HOG(X_train)\n",
    "                X_val_features, _   = preprocess_and_extract_features_HOG(X_val)\n",
    "            # Train and test the classifier on the validation data for this fold\n",
    "            clf              = clf.fit(X_train_features,y_train)\n",
    "            cpred            = clf.predict_proba(X_val_features)\n",
    "        else:\n",
    "            # extract features depends on the method chose\n",
    "            if f == 0:\n",
    "                X_train_features = preprocess_and_extract_features(X_train)\n",
    "                X_val_features   = preprocess_and_extract_features(X_val)\n",
    "            if f == 1:\n",
    "                _, X_train_features = preprocess_and_extract_features_HOG(X_train)\n",
    "                _, X_val_features   = preprocess_and_extract_features_HOG(X_val)\n",
    "            # Train and test the classifier on the validation data for this fold\n",
    "            clf              = clf.fit(X_train_features,y_train)\n",
    "            cpred            = clf.predict(X_val_features)\n",
    "        \n",
    "        # Save the predictions for this fold\n",
    "        prediction_scores[val_index] = cpred[:,1]\n",
    "    return prediction_scores\n",
    "\n",
    "def plot_roc(labels, prediction_scores):\n",
    "    fpr, tpr, _ = metrics.roc_curve(labels, prediction_scores, pos_label=1)\n",
    "    auc = metrics.roc_auc_score(labels, prediction_scores)\n",
    "    legend_string = 'AUC = {:0.3f}'.format(auc)\n",
    "   \n",
    "    plt.plot([0,1],[0,1],'--', color='gray', label='Chance')\n",
    "    plt.plot(fpr, tpr, label=legend_string)\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.grid('on')\n",
    "    plt.axis('square')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Sample script for cross validated performance\n",
    "'''\n",
    "# Set parameters for the analysis\n",
    "num_training_folds = 3\n",
    "\n",
    "# Load the data\n",
    "data, labels = load_data(dir_train_images, dir_train_labels, training=True)\n",
    "\n",
    "# Choose which classifier to use\n",
    "# 0: KNN, 1: CNN, 2: SVC\n",
    "clf = set_classifier(2)\n",
    "\n",
    "# Perform cross validated performance assessment\n",
    "# 0: mean&std, 1: HOG\n",
    "prediction_scores = cv_performance_assessment(data,labels,num_training_folds,clf,1)\n",
    "\n",
    "# Compute and plot the ROC curves\n",
    "plot_roc(labels, prediction_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Sample script for producing a Kaggle submission\n",
    "'''\n",
    "\n",
    "produce_submission = True # Switch this to True when you're ready to create a submission for Kaggle\n",
    "\n",
    "if produce_submission:\n",
    "    # Load data\n",
    "    training_data, training_labels = load_data(dir_train_images, dir_train_labels, training=True)\n",
    "    test_data, ids = load_data(dir_test_images, dir_test_ids, training=False)\n",
    "    \n",
    "    # let \"test_scores\" be outside of if function\n",
    "    test_scores = 0\n",
    "    \n",
    "    # choose model\n",
    "    # 0: KNN, 1: CNN, 2: SVC\n",
    "    c = 0\n",
    "    \n",
    "    # choose the feature extraction\n",
    "    #0: mean&std, 1: HOG\n",
    "    feature = 1\n",
    "    \n",
    "    # base on the model type to decide the method to extract features\n",
    "    if (c == 0) | (c == 2):\n",
    "        if feature == 0:\n",
    "            training_features = preprocess_and_extract_features(training_data)\n",
    "            test_features  = preprocess_and_extract_features(test_data)\n",
    "        if feature == 1:\n",
    "            training_features, _ = preprocess_and_extract_features_HOG(training_data)\n",
    "            test_features, _  = preprocess_and_extract_features_HOG(test_data)\n",
    "            \n",
    "        clf                            = set_classifier(c)\n",
    "        clf.fit(training_features, training_labels)\n",
    "        test_scores    = clf.predict_proba(test_features)[:,1]\n",
    "        # Save the predictions to a CSV file for upload to Kaggle\n",
    "        submission_file = pd.DataFrame({'id':    ids,\n",
    "                                   'score':  test_scores})\n",
    "        submission_file.to_csv('submission.csv',\n",
    "                                columns=['id','score'],\n",
    "                                index=False)\n",
    "\n",
    "    if c ==1: # inputs when using this model are transformed images\n",
    "        if feature == 0:\n",
    "            training_features = np.mean(training_data, axis =3).reshape(len(training_data),101,101,1)\n",
    "            test_features = np.mean(test_data, axis =3).reshape(len(test_data),101,101,1)\n",
    "        if feature == 1:\n",
    "            _, training_features = preprocess_and_extract_features_HOG(training_data)\n",
    "            _, test_features  = preprocess_and_extract_features_HOG(test_data)\n",
    "            \n",
    "        clf                            = set_classifier(c)\n",
    "        training_labels = to_categorical(training_labels)\n",
    "        clf.fit(training_features, training_labels, epochs=3)\n",
    "        test_scores = clf.predict(test_features)\n",
    "        # Save the predictions to a CSV file for upload to Kaggle\n",
    "        submission_file = pd.DataFrame({'id':    ids,\n",
    "                                   'score':  test_scores[:,1]})\n",
    "        submission_file.to_csv('submission.csv',\n",
    "                               columns=['id','score'],\n",
    "                               index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
